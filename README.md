# LLM Game Benchmark
This repository is developed to evaluate Large Language Models (LLMs) via Game Playing. It includes the following components:
- A leaderboard to view and compare the results of previous games among LLMs. We **welcome submissions** to the leaderboard.
  - Link to the Leaderboard webpage
- An extensible game simulation software to test LLMs via games such as Tic-Tac-Toe, Connect Four, and Gomoku
  - Link to the Game Simulation webpage
- The datasets to analyze the details of the games that are presented on the leaderboard
  - Link to the datasets

This repository **welcomes contributions and suggestions**. The LLM Game Benchmark repository is shared under the MIT License.

| Tic-Tac-Toe  | Connect Four | Gomoku |
| ------------- | ------------- | ------------- |
| ![tictactoe](https://github.com/research-outcome/LLM-Game-Benchmark/assets/1295373/bceee748-f151-4854-a558-a07dde7ff6a3)  | ![connect4](https://github.com/research-outcome/LLM-Game-Benchmark/assets/1295373/42f19aca-7c54-4813-ae0d-58f21b233b5b)  | ![gomoku](https://github.com/research-outcome/LLM-Game-Benchmark/assets/1295373/e79fdfc5-8acb-41bf-8237-acc9c720a90f) |

We have published the details of this study. If you use the output data, please cite the publication:
- Link to the publication

Use the following tags when made public:
#llm #game #benchmark #large-language-models #deep-learning #nlp #agi #reasoning #strategic-thinking #performance-testing #evaluation

